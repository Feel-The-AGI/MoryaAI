{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7759763,"sourceType":"datasetVersion","datasetId":4538026},{"sourceId":7788323,"sourceType":"datasetVersion","datasetId":4558639}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade h5py\n!pip install --upgrade typing-extensions\n!pip install --upgrade wheel","metadata":{"_uuid":"a1253181-0b1b-4eba-847f-42a6b0765f52","_cell_guid":"acdd5569-1083-4621-b59a-798c2efe472c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade transformers torch tqdm","metadata":{"_uuid":"a2798ef4-878b-4f5e-a2d0-5e7a97c8cfa5","_cell_guid":"3e27a4bf-37a1-4c6c-9198-bd617731b175","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForCausalLM\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nimport torch\nfrom tqdm import tqdm","metadata":{"_uuid":"4e9a3e02-6c7d-4969-8b95-4e884f951b1c","_cell_guid":"f2e4fdda-4b89-4ad9-8aa6-99050b7e2203","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add files to system for direct calling of class\nsys.path.append('/kaggle/input/morya-92m')\nprint(sys.path)\n\nfrom train import ChatData","metadata":{"_uuid":"437e8394-3f86-485b-94f9-77a562c8687d","_cell_guid":"f7bbd131-ee56-47fc-985b-8240a79b4aca","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_path = './distil_morya'\nmodel = GPT2LMHeadModel.from_pretrained('gpt2') # this is where the connection issue is\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2') # this as well\ntokenizer.pad_token = tokenizer.eos_token\nspecial_tokens_dict = {'bos_token': '<startofstring>',\n                       'eos_token': '<endofstring>',\n                       'additional_special_tokens': ['<bot> :']}\nnum_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"_uuid":"a273c4dc-8f04-4108-b896-29a0c36b3493","_cell_guid":"4d554129-61ec-4803-b60b-2f6c879ec9aa","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset\nchatData = ChatData('/kaggle/input/morya-92m/custom_conversation_dataset.json', tokenizer)\nsample_data = chatData[0] # debug line\nprint(sample_data)","metadata":{"_uuid":"a02bf923-2b13-4652-b21b-bb242059a16e","_cell_guid":"62c37938-22e3-4a7a-ba32-558099cff513","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare DataLoader\ndata_loader = DataLoader(chatData, batch_size=2, shuffle=True)","metadata":{"_uuid":"8663fab0-9027-4662-8731-b2302b92826e","_cell_guid":"a3d541d3-9cfb-4c01-abe6-e28cabe6f4d8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.optim import AdamW","metadata":{"_uuid":"5269aa65-7385-45a0-ae77-5e421990f754","_cell_guid":"5955da18-95a1-45c9-bc32-562ac1db38fb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optimizer and Scheduler\noptimizer = AdamW(model.parameters(), lr=5e-5)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)","metadata":{"_uuid":"f8b21052-525c-4b7e-ad55-729514a8fbaf","_cell_guid":"fc8d3c14-9d01-4cbf-8e84-710026f07e93","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.cuda.amp import autocast, GradScaler","metadata":{"_uuid":"8970a27b-0e88-42bd-8607-17627d5927c4","_cell_guid":"ccfd75b1-8a15-4aab-a980-ed05136d21ed","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade torch torchvision torchaudio --user","metadata":{"_uuid":"09e9784b-621a-4401-9797-20a509e411e7","_cell_guid":"3d312f2b-d5be-4cd7-90d2-50f1f99d0fae","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install PyTorch with CUDA support\n!pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 --user -f https://download.pytorch.org/whl/torch_stable.html","metadata":{"_uuid":"1e09c8e3-19d2-4522-8273-22db40ea262a","_cell_guid":"ae4775ae-1b37-4f57-9744-1280f36060d4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.__version__)\nprint(torch.cuda.is_available())\nprint(torch.version.cuda)\nprint(torch.backends.cudnn.version())","metadata":{"_uuid":"b24e6ddd-dcfb-439b-9318-db61e6ee1291","_cell_guid":"abcccf42-95c5-44e9-b935-4ab5b194bfa3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check CUDA availability and select device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif device.type == 'cuda':\n    print('Using GPU:', torch.cuda.get_device_name(0))  # Print GPU name\nelse:\n    print('CUDA is not available. Using CPU.')\nmodel = model.to(device)","metadata":{"_uuid":"8b08388c-d867-4495-ab61-3fbbaaaf4f37","_cell_guid":"d28e8fcd-fb7d-437d-99a3-6558e3399686","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correct device setup\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    print('Using GPU:', torch.cuda.get_device_name(0))  # Print GPU name\nelse:\n    device = torch.device('cpu')\n    print('CUDA is not available. Using CPU.')\noptimizer = Adam(model.parameters(), lr=5e-5)","metadata":{"_uuid":"dfefeed0-92e9-4802-b99a-90e763103ee3","_cell_guid":"14c73d25-62cc-4482-b47b-c55db063ec5d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb=512'","metadata":{"_uuid":"1a5d4dcf-43c6-4ea4-81cb-f47b7ffdaa0d","_cell_guid":"08067602-7b84-4d7b-84b6-ab1c0ef25e9d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Model memory size: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB\")\nprint(f\"Optimizer memory size: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB\")","metadata":{"_uuid":"2496a17c-da2d-4b0f-9e5b-0223e37b56dc","_cell_guid":"f11797f8-2949-44e4-b92c-9f3dc880466c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"_uuid":"ffd9c7f4-8a49-4dec-97fb-1be1b2ee7f60","_cell_guid":"58223c83-fdfe-49c0-b226-2b9feadf6ea9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Model memory size: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB\")\nprint(f\"Optimizer memory size: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB\")","metadata":{"_uuid":"50f41ca7-63f6-45df-bc6d-973403a4e70c","_cell_guid":"27046741-9882-4e8c-b149-098b07684286","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.memory_summary(device=None, abbreviated=False)","metadata":{"_uuid":"2f12da0c-d264-47d4-be06-beb77fff81d8","_cell_guid":"18589433-9d39-4e08-ac44-7b2a65f4fe9d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = GradScaler()\n\naccumulation_steps = 4  # Accumulate gradients over 4 steps\nnum_epochs = 6\nfor epoch in range(num_epochs + 1):\n    model.train()\n    total_loss = 0\n    progress_bar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f'Epoch {epoch+1}/{num_epochs}')\n\n    for batch_idx, batch in progress_bar:\n        input_ids, attention_mask = batch[0].to(device), batch[1].to(device)  # Move batch to GPU\n        labels = input_ids.detach().clone().to(device)  # Move labels to the same device\n\n        optimizer.zero_grad()\n\n        # Use autocast for mixed precision training\n        with autocast():\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n\n        # Backward pass scaled with scaler\n        scaler.scale(loss).backward()\n\n        # Perform optimizer step after accumulation_steps\n        if (batch_idx + 1) % accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n        # Print progress\n        if (batch_idx + 1) % 100 == 0:  # Adjust print frequency\n            current_loss = loss.item()\n            print(f'Epoch: {epoch+1}, Batch: {batch_idx+1}/{len(data_loader)}, Loss: {current_loss}')\n\n        # Clear GPU memory\n        del input_ids, attention_mask, outputs, loss, labels\n        torch.cuda.empty_cache()\n\n    avg_loss = total_loss / len(data_loader)\n    print(f'Epoch: {epoch+1}, Average Loss: {avg_loss}')","metadata":{"_uuid":"19912119-8ec6-4f10-a3aa-4eb1dd29eaef","_cell_guid":"db2e4f9b-309f-4905-a89a-7705bc76f58e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"TRANSFORMERS_USE_SAFETENSORS\"] = \"0\"\n\nmodel.save_pretrained('/kaggle/working')\ntokenizer.save_pretrained('/kaggle/working')","metadata":{"_uuid":"ebcfa11e-7ff1-4541-a3c0-fc2f585763f1","_cell_guid":"ab7e1af9-5e31-45a0-b4d2-4c2d085a5b32","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}